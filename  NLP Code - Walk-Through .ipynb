{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing on Billboard 100 Songs\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of my project is to predict the group (Top 25, Top 25 - 50, Top 50 - 75, Top 75 -100) a song in the Billboard 100 belongs to, based on the lyrics of the song. Indeed, the ultimate questions is how much do lyrics and words matter, versus other features such as the rhythm and artist popularity?\n",
    "\n",
    "In order to investigate this questions various Natural language processing tools were used including CountVectozier, TF-IDF, Sentiment Analysis and Part of Speech Tagging. An additional feature explored was the repetitiveness of a song, using the Zopfli compression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    "- [Scraping the Data](#Scraping-the-Data)\n",
    "     - [Billboard100 Scrap](#Billboard100-Scrap)\n",
    "     - [Lyric Scrap](#Lyric-Scrap)\n",
    "     - [Genres Scrap](#Genres-Scrap)\n",
    "- [EDA](#EDA)\n",
    "     - [Genres Grouping](#Genres-Grouping)\n",
    "     - [Artist Popularity](#Artist-Popularity)\n",
    "- [ CountVectorizer Model](# CountVectorizer-Model)\n",
    "    - [CountVectorizer, Artist Popularity & Genre Model ](#CountVectorizer,-Artist-Popularity-&-Genre-Model)\n",
    "- [TF-IDF Model](#TF-IDF-Model)\n",
    "    - [TF-IDF, Artist Popularity & Genre Model ](#TF-IDF,-Artist-Popularity-&-Genre-Model)\n",
    "- [Sentiment Analysis](#Sentiment Analysis)\n",
    "    - [Sentiment KNN Model](#Sentiment KNN Model)\n",
    "    - [Compound Sentiment & Genres Model](#Compound-Sentiment-&-Genres-Model)\n",
    "- [Repetitiveness ](#Repetitiveness)\n",
    "- [Part of Speech Tagging ](#Part-of-Speech-Tagging )\n",
    "\t- [Percentage Repetition, Part of Speech Tags & Genre Model](#Percentage-Repetition,-Part-of-Speech-Tags-& Genre-Model)\n",
    "- [All Features Model](#Part-of-Speech-Tagging )\n",
    "- [Conclusion](#Conclusion )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Scraping the Data\"></a>\n",
    "## Scraping the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Billboard100 Scrap\"></a>\n",
    "### Billboard100 Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapping the data was a three-fold process. First, I needed to acquire the names of the songs, artists and rank of the songs in Billboard 100 list. The Billboard 100 is a weekly released list of the top 100 most popular songs.  No duplicate songs could be present in the data as this would result in songs being classified into multiple groups, due to the change in rank of a song on the list over time. To guarantee there were no duplicate songs I did a grouped-by with the artist and song, and selected the minimum rank position (ie. the highest position in the chart). The songs were taken from the time period of August 2018 to January 2016. \n",
    "\n",
    "For the number one ranked songs in the Billboard 100, the HTML construct of the page was different that resulted in a separate scrap for these songs (second code below). \n",
    "\n",
    "Using the rank of the songs, I created my groups that were my target variable. The groups were Top 25, Top 25 - 50, Top 50 - 75 and Top 75 -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SONGS from Rank 2 -100 \n",
    "def extract_song(listing):\n",
    "    try:\n",
    "        song =listing.find(\"span\",attrs={\"class\":\"chart-list-item__title-text\"}).get_text()\n",
    "        return song \n",
    "    \n",
    "    except:\n",
    "        return \"Not found\"\n",
    "\n",
    "def extract_artist(listing):\n",
    "    try:\n",
    "        artist =listing.find(\"div\",attrs={\"class\":\"chart-list-item__artist\"}).get_text()\n",
    "        return artist \n",
    "    \n",
    "    except:\n",
    "        return \"Not found\"\n",
    "\n",
    "def extract_rank(listing):\n",
    "    try:\n",
    "        rank =listing.find(\"div\",attrs={\"class\":\"chart-list-item__rank\"}).get_text()\n",
    "        return rank \n",
    "    \n",
    "    except:\n",
    "        return \"Not found\"\n",
    "\n",
    "url_template = \"https://www.billboard.com/charts/hot-100/{}\"\n",
    "\n",
    "songs = []\n",
    "artists = []\n",
    "ranks = []\n",
    "\n",
    "for date in set(['2018-08-04', '2018-07-28', '2018-07-21', '2018-07-14', '2018-07-07', \n",
    "    '2018-06-30', '2018-06-23', '2018-06-16', '2018-06-09', '2018-06-02','2018-05-26',\n",
    "                 '2018-05-19','2018-05-12','2018-05-05','2018-04-28','2018-04-21',\n",
    "                '2018-04-14','2018-04-07','2018-03-31','2018-03-24','2018-03-17',\n",
    "                '2018-03-10','2018-03-03','2018-02-24','2018-02-17','2018-02-10',\n",
    "                '2018-02-03','2018-01-27','2018-01-20','2018-01-13','2018-01-06','2017-12-30'\n",
    "                '2017-12-23','2017-12-16','2017-12-09','2017-12-02','2017-11-25','2017-11-18','2017-11-11'\n",
    "                '2017-11-04','2017-10-28','2017-10-21','2017-10-14','2017-10-07','2017-09-30','2017-09-23',\n",
    "                '2017-09-16','2017-09-09','2017-09-02','2017-08-26','2017-08-19','2017-08-12','2017-08-05','2017-07-29', '2017-07-22','2017-07-15','2017-07-08','2017-07-01','2017-06-24','2017-06-17',\n",
    "                '2017-06-10','2017-06-03','2017-05-27','2017-05-20','2017-05-13','2017-05-06','2017-04-29',\n",
    "                '2017-04-22','2017-04-15','2017-04-08','2017-04-01','2017-03-25','2017-03-28','2017-03-11',\n",
    "                '2017-03-18','2017-03-11','2017-03-04','2017-02-25','2017-02-18','2017-02-11','2017-02-04',\n",
    "                '2017-01-28','2017-01-21','2017-01-14','2017-01-07','2016-12-31','2016-12-24','2016-12-17',\n",
    "                 '2016-12-10','2016-12-03','2016-11-26','2016-11-19','2016-11-12','2016-11-05','2016-10-29',\n",
    "                '2016-10-22','2016-10-15','2016-10-08','2016-10-01','2016-09-24','2016-09-17','2016-09-10',\n",
    "                '2016-09-03','2016-08-27','2016-08-20','2016-08-13','2016-08-06','2016-07-30','2016-07-23','2016-07-23',\n",
    "                '2016-07-16','2016-07-09','2016-07-02','2016-06-25','2016-06-18','2016-06-11','2016-06-04','2016-05-28'\n",
    "                ,'2016-05-21','2016-05-14','2016-05-07','2016-04-30','2016-04-23','2016-04-16','2016-04-09','2016-04-02',\n",
    "                '2016-03-26','2016-03-19','2016-03-12','2016-03-05','2016-02-27','2016-02-20','2016-02-13','2016-02-06',\n",
    "                '2016-01-30','2016-01-23','2016-01-16','2016-01-09','2016-01-02']):\n",
    "    updated_url = url_template.format(date)                             \n",
    "    jobs_request = requests.get(updated_url)\n",
    "    soup = BeautifulSoup(jobs_request.text,\"html.parser\")\n",
    "    listings = soup.find_all(\"div\",attrs={\"class\":\"chart-list-item__first-row\"})\n",
    "    \n",
    "    \n",
    "    for listing in listings:\n",
    "        songs.append(extract_song(listing))\n",
    "        artists.append(extract_artist(listing))\n",
    "        ranks.append(extract_rank(listing))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SONGS from Rank 1 \n",
    "def extract_top_song(listing):\n",
    "    try:\n",
    "        top_song =listing.find(\"div\",attrs={\"class\":\"chart-number-one__title\"}).get_text()\n",
    "        return top_song \n",
    "    \n",
    "    except:\n",
    "        return \"Not found\"\n",
    "\n",
    "def extract_top_artist(listing):\n",
    "    try:\n",
    "        top_artist =listing.find(\"div\",attrs={\"class\":\"chart-number-one__artist\"}).get_text()\n",
    "        return top_artist\n",
    "    \n",
    "    except:\n",
    "        return \"Not found\"\n",
    "    \n",
    "    \n",
    "url_template = \"https://www.billboard.com/charts/hot-100/{}\"\n",
    "\n",
    "top_songs = []\n",
    "top_artists = []\n",
    "\n",
    "for date in set(['2018-08-04', '2018-07-28', '2018-07-21', '2018-07-14', '2018-07-07', \n",
    "    '2018-06-30', '2018-06-23', '2018-06-16', '2018-06-09', '2018-06-02','2018-05-26',\n",
    "                 '2018-05-19','2018-05-12','2018-05-05','2018-04-28','2018-04-21',\n",
    "                '2018-04-14','2018-04-07','2018-03-31','2018-03-24','2018-03-17',\n",
    "                '2018-03-10','2018-03-03','2018-02-24','2018-02-17','2018-02-10',\n",
    "                '2018-02-03','2018-01-27','2018-01-20','2018-01-13','2018-01-06','2017-12-30'\n",
    "                '2017-12-23','2017-12-16','2017-12-09','2017-12-02','2017-11-25','2017-11-18','2017-11-11'\n",
    "                '2017-11-04','2017-10-28','2017-10-21','2017-10-14','2017-10-07','2017-09-30','2017-09-23',\n",
    "                '2017-09-16','2017-09-09','2017-09-02','2017-08-26','2017-08-19','2017-08-12','2017-08-05','2017-07-29', '2017-07-22','2017-07-15','2017-07-08','2017-07-01','2017-06-24','2017-06-17',\n",
    "                '2017-06-10','2017-06-03','2017-05-27','2017-05-20','2017-05-13','2017-05-06','2017-04-29',\n",
    "                '2017-04-22','2017-04-15','2017-04-08','2017-04-01','2017-03-25','2017-03-28','2017-03-11',\n",
    "                '2017-03-18','2017-03-11','2017-03-04','2017-02-25','2017-02-18','2017-02-11','2017-02-04',\n",
    "                '2017-01-28','2017-01-21','2017-01-14','2017-01-07','2016-12-31','2016-12-24','2016-12-17',\n",
    "                 '2016-12-10','2016-12-03','2016-11-26','2016-11-19','2016-11-12','2016-11-05','2016-10-29',\n",
    "                '2016-10-22','2016-10-15','2016-10-08','2016-10-01','2016-09-24','2016-09-17','2016-09-10',\n",
    "                '2016-09-03','2016-08-27','2016-08-20','2016-08-13','2016-08-06','2016-07-30','2016-07-23','2016-07-23',\n",
    "                '2016-07-16','2016-07-09','2016-07-02','2016-06-25','2016-06-18','2016-06-11','2016-06-04','2016-05-28'\n",
    "                ,'2016-05-21','2016-05-14','2016-05-07','2016-04-30','2016-04-23','2016-04-16','2016-04-09','2016-04-02',\n",
    "                '2016-03-26','2016-03-19','2016-03-12','2016-03-05','2016-02-27','2016-02-20','2016-02-13','2016-02-06',\n",
    "                '2016-01-30','2016-01-23','2016-01-16','2016-01-09','2016-01-02']):\n",
    "    updated_url = url_template.format(date)                             \n",
    "    jobs_request = requests.get(updated_url)\n",
    "    soup = BeautifulSoup(jobs_request.text,\"html.parser\")\n",
    "    listings = soup.find_all(\"div\",attrs={\"class\":\"chart-number-one__info \"})\n",
    "    \n",
    "    \n",
    "    for listing in listings:\n",
    "        top_songs.append(extract_top_song(listing))\n",
    "        top_artists.append(extract_top_artist(listing))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby to elimiate duplicate songs  \n",
    "\n",
    "result2 = result.groupby([\"Artist\",\"Song\"])[\"Rank\"].min().reset_index().set_index('Artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Groups from the Rank of the songs \n",
    "\n",
    "classifiers = []\n",
    "\n",
    "for entry in result2[\"Rank\"]:\n",
    "    if int(entry) <= 25:\n",
    "        classifiers.append(\"Top 25\")\n",
    "    elif int(entry) <= 50 and int(entry)>25:\n",
    "        classifiers.append(\"Top 25 - 50\")\n",
    "    elif int(entry) <= 75 and int(entry)>50:\n",
    "        classifiers.append(\"Top 50 - 75\")\n",
    "    else: \n",
    "        classifiers.append(\"Top 75- 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Lyric Scrap\"></a>\n",
    "### Lyric Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that none of the songs were duplicated, I was able to scarp the lyrics from Lyrics Wiki. The URL format was  \"Artist_Song\". With multiple artist on a track or featuring artist the correct URL format for the song lyrics was more complicated. In order to have the same Artist, Song combination as that on LyricsWiki I had to go through the songs consecutively, checking to see which artist the song was listed under. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_lyrics (lyric):\n",
    "    \n",
    "    cleaned = str(lyric).split(\"<br/>\")\n",
    "\n",
    "    #line1 \n",
    "    line1 = cleaned[0] \n",
    "    new_line1 = line1[22:]\n",
    "    cleaned[0] = cleaned[0].replace(cleaned[0],new_line1)\n",
    "\n",
    "    #lastline\n",
    "    last_index = len(cleaned)-1\n",
    "    lastline = cleaned[last_index]\n",
    "\n",
    "    len_lastline = len(lastline)\n",
    "    index = len_lastline -38\n",
    "    new_lastline = lastline[:index]\n",
    "\n",
    "    cleaned[last_index] = cleaned[last_index].replace(cleaned[last_index],new_lastline)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://lyrics.wikia.com/wiki/{}:{}\"\n",
    "\n",
    "lyrics_clean= []\n",
    "\n",
    "for entry in zipped:\n",
    "    artist = entry[0]\n",
    "    song = entry[1]\n",
    "    updated_url = url_template.format(artist,song)  \n",
    "    #print(updated_url)\n",
    "    lyric_request= requests.get(updated_url)\n",
    "    soup = BeautifulSoup(lyric_request.text,\"html.parser\")\n",
    "    \n",
    "    \n",
    "    lyrics = soup.find_all(\"div\",attrs={\"class\":\"lyricbox\"})\n",
    "    #print(lyrics)\n",
    "    \n",
    "    if lyrics != []:\n",
    "    \n",
    "        for lyric in lyrics:\n",
    "            lyrics_clean.append(cleaned_lyrics(lyric))\n",
    "            \n",
    "        \n",
    "    else: \n",
    "        lyrics_clean.append(\"Not Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Genres Scrap\"></a>\n",
    "### Genres Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I needed the genre of the artist. In order to acquire the genre I used Every Noise, which returns up to 10 genres for a single artist. The URL format was simply the artist name. With multiple artist on the song, the genre of the main artist was found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_genre(genres):\n",
    "    try:\n",
    "        g1 = genres.get_text().strip(\"\\n\")\n",
    "        return g1\n",
    "    \n",
    "    except:\n",
    "        return \"Not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://everynoise.com/lookup.cgi?who={}&mode=map\"\n",
    "\n",
    "genres_everyn = []\n",
    "\n",
    "for artist in artist_add:\n",
    "    updated_url = url_template.format(artist)   \n",
    "    #print(updated_url)\n",
    "    jobs_request = requests.get(updated_url)\n",
    "    soup = BeautifulSoup(jobs_request.text,\"html.parser\")\n",
    "    genres = soup.find(\"div\")\n",
    "    \n",
    "    genres_everyn.append(extract_genre(genres))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: At every section of the scraping process (Billboard 100, Lyrics, Genres) the data was assembled into a DataFrames. These DataFrames were then finally merged into one single DataFrame, where all the information was present for every song. \n",
    "\n",
    "The DataFrame has the columns: Artist, Song, Lyrics, Rank, Group , Genres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"EDA\"></a>\n",
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data was assembled it was time to clean it. This entailed removing any strange symbols, making all the letters lowercase and finally removing any foreign songs that did not use the English alphabet. Once all of the data was cleaned, graphs were created in order to further explore the data and gain a deeper understanding. \n",
    "(Note: To see the final graphs, please see the \"NLP on Billboard 100 Presenation\" PDF in the repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Genre Grouping\"></a>\n",
    "### Genre Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within EDA two additional features were created from the scrapped data; genres and artist popularity. The genres scraped from EveryNoise were very specific ranging upto 10 genres for a single artist. For the genres to have any weight within the model, wider genres groups needed to be created. For example, for the Artist \"Drake\" Canadian pop and pop were listed as genres, so these categories were grouped under pop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_g2 = []\n",
    "\n",
    "for entry in df[\"EV_cleaned_G2\"]:\n",
    "    if \"pop\" in entry:\n",
    "        general_g2.append(\"pop\")\n",
    "    elif \"country\" in entry:\n",
    "        general_g2.append(\"country\")\n",
    "    elif \"rap\" in entry:\n",
    "        general_g2.append(\"rap\")\n",
    "    elif \"trap\" in entry:\n",
    "        general_g2.append(\"trap\")\n",
    "    elif \"r&b\" in entry:\n",
    "        general_g2.append(\"r&b\")\n",
    "    elif \"indie\" in entry:\n",
    "        general_g2.append(\"indie\")\n",
    "    elif \"indie\" in entry:\n",
    "        general_g2.append(\"stomp and holler\")\n",
    "    elif \"punk\" in entry:\n",
    "        general_g2.append(\"punk\")\n",
    "    elif \"hip hop\" in entry:\n",
    "        general_g2.append(\"hip hop\")\n",
    "    elif \"edm\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \"metal\" in entry:\n",
    "        general_g2.append(\"metal\")\n",
    "    elif \"reggaeton\" in entry:\n",
    "        general_g2.append(\"latin\")\n",
    "    elif \"reggae fusion\" in entry:\n",
    "        general_g2.append(\"reggae\")\n",
    "    elif \"alternative\" in entry:\n",
    "        general_g2.append(\"alternative\")\n",
    "    elif \"tropical\" in entry:\n",
    "        general_g2.append(\"tropical\")\n",
    "    elif \"rock\" in entry:\n",
    "        general_g2.append(\"rock\")\n",
    "    elif \"boy band\" in entry:\n",
    "        general_g2.append(\"pop\")\n",
    "    elif \"red dirt\" in entry:\n",
    "        general_g2.append(\"country\")\n",
    "    elif \"singer-songwriter\" in entry:\n",
    "        general_g2.append(\"country\") \n",
    "    elif \"a cappella\" in entry:\n",
    "        general_g2.append(\"country\") \n",
    "    elif \"bolero\" in entry:\n",
    "        general_g2.append(\"latin\") \n",
    "    elif \"bachata\" in entry:\n",
    "        general_g2.append(\"latin\") \n",
    "    elif \"g funk\" in entry:\n",
    "        general_g2.append(\"r&b\")\n",
    "    elif \"neo mellow\" in entry:\n",
    "        general_g2.append(\"pop\")\n",
    "    elif \"new romantic\" in entry:\n",
    "        general_g2.append(\"pop\")\n",
    "    elif \"neo soul\" in entry:\n",
    "        general_g2.append(\"soul\")\n",
    "    elif \"permanent wave\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \"aussietronica\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \"moombahton\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \"brostep\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \"adult standards\" in entry:\n",
    "        general_g2.append(\"traditional pop\")\n",
    "    elif \"anthem worship\" in entry:\n",
    "        general_g2.append(\"christian music\")\n",
    "    elif \"hollywood\" in entry:\n",
    "        general_g2.append(\"pop\")\n",
    "    elif \"lift kit\" in entry:\n",
    "        general_g2.append(\"country\")\n",
    "    elif \"stomp and holler\" in entry:\n",
    "        general_g2.append(\"country\")\n",
    "    elif \"show tunes\" in entry:\n",
    "        general_g2.append(\"musicals\")\n",
    "    elif \"disney\" in entry:\n",
    "        general_g2.append(\"musicals\")\n",
    "    elif \"motown\" in entry:\n",
    "        general_g2.append(\"musicals\")\n",
    "    elif \"uk funky\" in entry:\n",
    "        general_g2.append(\"reggae\")\n",
    "    elif \"talent show\" in entry:\n",
    "        general_g2.append(\"tv music\") \n",
    "    elif \"deep talent show\" in entry:\n",
    "        general_g2.append(\"tv music\")  \n",
    "    elif \"downtempo\" in entry:\n",
    "        general_g2.append(\"soul\")  \n",
    "    elif \"drill\" in entry:\n",
    "        general_g2.append(\"trap\")  \n",
    "    elif \"erotica\" in entry:\n",
    "        general_g2.append(\"r&b\")\n",
    "    elif \"indonesian jazz\" in entry:\n",
    "        general_g2.append(\"jazz\")\n",
    "    elif \"tropical\" in entry:\n",
    "        general_g2.append(\"latin\")\n",
    "    elif \"australian dance\" in entry:\n",
    "        general_g2.append(\"pop\")\n",
    "    elif \"progressive electro house\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \" latin\" in entry:\n",
    "        general_g2.append(\"latin\")\n",
    "    elif \"electronica\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \" idol\" in entry:\n",
    "        general_g2.append(\"tv music\")\n",
    "    elif \"canadian folk\" in entry:\n",
    "        general_g2.append(\"folk\")\n",
    "    elif \"musical\" in entry:\n",
    "        general_g2.append(\"musicals\")\n",
    "    elif \" big room\" in entry:\n",
    "        general_g2.append(\"pop\")\n",
    "    elif \" deep euro house\" in entry:\n",
    "        general_g2.append(\"electronic\")\n",
    "    elif \" ccm\" in entry:\n",
    "        general_g2.append(\"alternative\")\n",
    "    elif \" neo-psychedelic\" in entry:\n",
    "        general_g2.append(\"rock\")\n",
    "    elif \" movie tunes\" in entry:\n",
    "        general_g2.append(\"tv music\")\n",
    "    elif \" emo\" in entry:\n",
    "        general_g2.append(\"rock\")\n",
    "    else:\n",
    "        general_g2.append(entry)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Artist Popularity\"></a>\n",
    "### Artist Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artist popularity was created by the counting number of times an artist appeared in each group. Every time the artist appeared within the group their count was increased. This created four new columns of Artist Popularity for each respective group (Top 25, Top 25 - 50, Top 50 - 75, Top 75 -100). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Artist Popularity for Artist in the Top 25 group\n",
    "\n",
    "twentyone_Savage = 0\n",
    "five_Seconds_Of_Summer = 0\n",
    "Adele = 0\n",
    "Alessia_Cara= 0\n",
    "Amine= 0\n",
    "Ariana_Grande= 0\n",
    "Ayo_Teo= 0\n",
    "Bazzi= 0\n",
    "Bebe_Rexha_and_Florida_Georgia_Line= 0\n",
    "Beyonce= 0\n",
    "Big_Sean = 0\n",
    "BlocBoy_JB= 0\n",
    "Brett_Young= 0\n",
    "Britney_Spears= 0\n",
    "Bruno_Mars= 0\n",
    "Bruno_Mars_and_Cardi_B= 0\n",
    "Bryson_Tiller= 0\n",
    "Calvin_Harris = 0\n",
    "Camila_Cabello = 0\n",
    "Cardi_B = 0\n",
    "Charlie_Puth= 0\n",
    "Childish_Gambino= 0\n",
    "Chris_Brown= 0\n",
    "Clean_Bandit= 0\n",
    "Coldplay= 0\n",
    "DRAM= 0\n",
    "DJ_Khaled= 0\n",
    "DJ_Snake= 0\n",
    "DNCE= 0\n",
    "Dan_and_Shay= 0\n",
    "David_Guetta= 0\n",
    "Daya= 0\n",
    "Demi_Lovato= 0\n",
    "Desiigner= 0\n",
    "Drake= 0\n",
    "Drake_and_Future= 0\n",
    "Dua_Lipa= 0\n",
    "Ed_Sheeran= 0\n",
    "Ella_Mai= 0\n",
    "Elle_King= 0\n",
    "Ellie_Goulding= 0\n",
    "Eminem= 0\n",
    "Fetty_Wap= 0\n",
    "Fifth_Harmony= 0\n",
    "Flo_Rida= 0\n",
    "Florida_Georgia_Line= 0\n",
    "Flume= 0\n",
    "French_Montana= 0\n",
    "Future= 0\n",
    "G_Eazy= 0\n",
    "G_Eazy_and_Halsey = 0\n",
    "G_Eazy_x_Bebe_Rexha = 0\n",
    "Gucci_Mane = 0\n",
    "Hailee_Steinfeld_and_Grey = 0\n",
    "Halsey= 0\n",
    "Harry_Styles= 0\n",
    "Imagine_Dragons= 0\n",
    "J_Balvin_and_Willy_William= 0\n",
    "J_Cole= 0\n",
    "James_Arthur= 0\n",
    "James_Bay= 0\n",
    "Jeremih= 0\n",
    "John_Legend= 0\n",
    "Jon_Bellion= 0\n",
    "Jordan_Smith= 0\n",
    "Juice_WRLD= 0\n",
    "Julia_Michaels= 0\n",
    "Justin_Bieber= 0\n",
    "Justin_Timberlake= 0\n",
    "KYLE= 0\n",
    "Kane_Brown= 0\n",
    "Kanye_West= 0\n",
    "Katy_Perry= 0\n",
    "Keith_Urban= 0\n",
    "Kelly_Clarkson= 0\n",
    "Kendrick_Lamar= 0\n",
    "Kendrick_Lamar_and_SZA= 0\n",
    "Kent_Jones= 0\n",
    "Kesha= 0\n",
    "Kevin_Gates= 0\n",
    "Khalid= 0\n",
    "Khalid_and_Normani= 0\n",
    "Kiiara= 0\n",
    "Kodak_Black= 0\n",
    "Kygo_and_Selena_Gomez= 0\n",
    "Lady_Gaga= 0\n",
    "Liam_Payne= 0\n",
    "Lil_Dicky= 0\n",
    "Lil_Pump= 0\n",
    "Lil_Uzi_Vert= 0\n",
    "Lil_Wayne= 0\n",
    "Lin_Manuel_Miranda= 0\n",
    "Logic= 0\n",
    "Lorde= 0\n",
    "Luis_Fonsi_and_Daddy_Yankee= 0\n",
    "Lukas_Graham= 0\n",
    "MAX= 0\n",
    "Machine_Gun_Kelly_and_Camila_Cabello= 0\n",
    "Major_Lazer= 0\n",
    "Major_Lazer_and_DJ_Snake= 0\n",
    "Mariah_Carey= 0\n",
    "Marian_Hill= 0\n",
    "Mark_Ronson= 0\n",
    "Maroon_5= 0\n",
    "Marshmello_and_Anne_Marie = 0\n",
    "Martin_Garrix_and_Bebe_Rexha= 0\n",
    "Meghan_Trainor= 0\n",
    "Migos= 0\n",
    "Mike_Posner= 0\n",
    "Miley_Cyrus= 0\n",
    "NF= 0\n",
    "Niall_Horan= 0\n",
    "Nick_Jonas= 0\n",
    "Nicki_Minaj= 0\n",
    "Offset_and_Metro_Boomin= 0\n",
    "Pink= 0\n",
    "Pentatonix= 0\n",
    "Portugal_The_Man= 0\n",
    "Post_Malone= 0\n",
    "Prince= 0\n",
    "Prince_And_The_Revolution= 0\n",
    "Rae_Sremmurd= 0\n",
    "Rihanna= 0\n",
    "Ruth_B= 0\n",
    "Sam_Hunt= 0\n",
    "Sam_Smith= 0\n",
    "Selena_Gomez= 0\n",
    "Shawn_Mendes= 0\n",
    "Shawn_Mendes_and_Camila_Cabello= 0\n",
    "Sia= 0\n",
    "Taylor_Swift= 0\n",
    "The_Carters= 0\n",
    "The_Chainsmokers= 0\n",
    "The_Chainsmokers_and_Coldplay= 0\n",
    "The_Weeknd = 0\n",
    "The_Weeknd_and_Kendrick_Lamar= 0\n",
    "Thomas_Rhett = 0\n",
    "Tory_Lanez= 0\n",
    "Travis_Scott= 0\n",
    "Troye_Sivan= 0\n",
    "Tyga= 0\n",
    "X_Ambassadors= 0\n",
    "XXXTENTACION= 0\n",
    "Yo_Gotti= 0\n",
    "Young_MA= 0\n",
    "Zara_Larsson_and_MNEK= 0\n",
    "Zay_Hilfigerrr_and_Zayion_McCall= 0\n",
    "Zayn= 0\n",
    "Zayn_and_Taylor_Swift= 0\n",
    "Zedd= 0\n",
    "Zedd_and_Alessia_Cara= 0\n",
    "gnash= 0\n",
    "twenty_one_pilots = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for entry in group25[\"Artist_cleaned\"]:\n",
    "    if entry == '21 Savage': \n",
    "        twentyone_Savage +=1  \n",
    "    elif entry == '5 Seconds Of Summer':\n",
    "        five_Seconds_Of_Summer+=1\n",
    "    elif entry == 'Adele':\n",
    "        Adele +=1  \n",
    "    elif entry == 'Alessia Cara':\n",
    "        Alessia_Cara +=1 \n",
    "    elif entry == 'Amine':\n",
    "        Amine +=1  \n",
    "    elif entry == 'Ariana Grande':\n",
    "        Ariana_Grande +=1  \n",
    "    elif entry == 'Ayo & Teo':\n",
    "        Ayo_Teo +=1   \n",
    "        \n",
    "    elif entry == 'Bazzi':\n",
    "        Bazzi +=1      \n",
    "    elif entry == 'Bebe Rexha & Florida Georgia Line':\n",
    "        Bebe_Rexha_and_Florida_Georgia_Line +=1      \n",
    "    elif entry == 'Beyonce':\n",
    "        Beyonce +=1      \n",
    "    elif entry == 'Big Sean':\n",
    "        Big_Sean +=1        \n",
    "    elif entry == 'BlocBoy JB':\n",
    "        BlocBoy_JB +=1 \n",
    "    elif entry == 'Brett Young':\n",
    "        Brett_Young +=1 \n",
    "    elif entry == 'Britney Spears':\n",
    "        Britney_Spears +=1 \n",
    "    elif entry == 'Bruno Mars':\n",
    "        Bruno_Mars +=1\n",
    "    elif entry == 'Bruno Mars & Cardi B':\n",
    "        Bruno_Mars_and_Cardi_B +=1 \n",
    "    elif entry == 'Bryson Tiller':\n",
    "        Bryson_Tiller +=1 \n",
    "    elif entry == 'Calvin Harris':\n",
    "        Calvin_Harris +=1\n",
    "    elif entry == 'Camila Cabello':\n",
    "        Camila_Cabello +=1\n",
    "    elif entry == 'Cardi B':\n",
    "        Cardi_B +=1\n",
    "    elif entry == 'Charlie Puth':\n",
    "        Charlie_Puth +=1\n",
    "    elif entry == 'Childish Gambino':\n",
    "        Childish_Gambino +=1\n",
    "    elif entry == 'Chris Brown':\n",
    "        Chris_Brown +=1\n",
    "    elif entry == 'Clean Bandit':\n",
    "        Clean_Bandit +=1\n",
    "    elif entry == 'Coldplay':\n",
    "        Coldplay +=1\n",
    "    elif entry == 'D.R.A.M.':\n",
    "        DRAM +=1\n",
    "    elif entry == 'DJ Khaled':\n",
    "        DJ_Khaled +=1\n",
    "    elif entry == 'DJ Snake':\n",
    "        DJ_Snake +=1\n",
    "    elif entry == 'DNCE':\n",
    "        DNCE +=1\n",
    "    elif entry == 'Dan + Shay':\n",
    "        Dan_and_Shay +=1\n",
    "        \n",
    "    elif entry == 'David Guetta':\n",
    "        David_Guetta +=1\n",
    "    elif entry == 'Daya':\n",
    "        Daya +=1\n",
    "    elif entry == 'Demi Lovato':\n",
    "        Demi_Lovato +=1\n",
    "    elif entry == 'Desiigner':\n",
    "        Desiigner +=1\n",
    "    elif entry == 'Drake':\n",
    "        Drake +=1\n",
    "    elif entry == 'Drake & Future':\n",
    "        Drake_and_Future +=1\n",
    "    elif entry == 'Dua Lipa':\n",
    "        Dua_Lipa +=1\n",
    "    elif entry == 'Ed Sheeran':\n",
    "        Ed_Sheeran +=1\n",
    "    elif entry == 'Ella Mai':\n",
    "        Ella_Mai +=1\n",
    "    elif entry == 'Elle King':\n",
    "        Elle_King +=1\n",
    "    elif entry == 'Ellie Goulding':\n",
    "        Ellie_Goulding +=1\n",
    "    elif entry == 'Eminem':\n",
    "        Eminem +=1\n",
    "    elif entry == 'Fetty Wap':\n",
    "        Fetty_Wap +=1\n",
    "    elif entry == 'Fifth Harmony':\n",
    "        Fifth_Harmony +=1\n",
    "    elif entry == 'Flo Rida':\n",
    "        Flo_Rida +=1\n",
    "    elif entry == 'Florida Georgia Line':\n",
    "        Florida_Georgia_Line +=1\n",
    "    elif entry == 'Flume':\n",
    "        Flume +=1\n",
    "    elif entry == 'French Montana':\n",
    "        French_Montana +=1\n",
    "    elif entry == 'Future':\n",
    "        Future +=1\n",
    "    elif entry == 'G-Eazy':\n",
    "        G_Eazy +=1\n",
    "    elif entry == 'G-Eazy & Halsey':\n",
    "        G_Eazy_and_Halsey +=1\n",
    "    elif entry == 'G-Eazy x Bebe Rexha':\n",
    "        G_Eazy_x_Bebe_Rexha +=1\n",
    "    elif entry == 'Gucci Mane':\n",
    "        Gucci_Mane +=1\n",
    "    elif entry == 'Hailee Steinfeld & Grey':\n",
    "        Hailee_Steinfeld_and_Grey +=1\n",
    "    \n",
    "    elif entry == 'Halsey':\n",
    "        Halsey +=1\n",
    "    elif entry == 'Harry Styles':\n",
    "        Harry_Styles +=1\n",
    "    elif entry == 'Imagine Dragons':\n",
    "        Imagine_Dragons +=1\n",
    "    elif entry == 'J Balvin & Willy William':\n",
    "        J_Balvin_and_Willy_William +=1\n",
    "    elif entry == 'J. Cole':\n",
    "        J_Cole +=1\n",
    "    elif entry == 'James Arthur':\n",
    "        James_Arthur +=1\n",
    "    elif entry == 'James Bay':\n",
    "        James_Bay +=1\n",
    "    \n",
    "    elif entry == 'Jeremih':\n",
    "        Jeremih +=1\n",
    "    elif entry == 'John Legend':\n",
    "        John_Legend +=1\n",
    "    elif entry == 'Jon Bellion':\n",
    "        Jon_Bellion +=1\n",
    "    elif entry == 'Jordan Smith':\n",
    "        Jordan_Smith +=1\n",
    "    elif entry == 'Juice WRLD':\n",
    "        Juice_WRLD +=1\n",
    "    elif entry == 'Julia Michaels':\n",
    "        Julia_Michaels +=1\n",
    "    elif entry == 'Justin Bieber':\n",
    "        Justin_Bieber +=1\n",
    "    elif entry == 'Justin Timberlake':\n",
    "        Justin_Timberlake +=1\n",
    "    elif entry == 'KYLE':\n",
    "        KYLE +=1 \n",
    "    elif entry == 'Kane Brown':\n",
    "        Kane_Brown +=1\n",
    "    elif entry == 'Kanye West':\n",
    "        Kanye_West +=1\n",
    "    elif entry == 'Katy Perry':\n",
    "        Katy_Perry +=1\n",
    "    elif entry == 'Keith Urban':\n",
    "        Keith_Urban +=1\n",
    "    elif entry == 'Kelly Clarkson':\n",
    "        Kelly_Clarkson +=1\n",
    "    elif entry == 'Kendrick Lamar':\n",
    "        Kendrick_Lamar +=1\n",
    "    elif entry == 'Kendrick Lamar & SZA':\n",
    "        Kendrick_Lamar_and_SZA +=1\n",
    "    elif entry == 'Kent Jones':\n",
    "        Kent_Jones +=1\n",
    "    elif entry == 'Kesha':\n",
    "        Kesha +=1\n",
    "    elif entry == 'Kevin Gates':\n",
    "        Kevin_Gates +=1\n",
    "    elif entry == 'Khalid':\n",
    "        Khalid +=1\n",
    "    elif entry == 'Khalid & Normani':\n",
    "         Khalid_and_Normani +=1\n",
    "    elif entry == 'Kiiara':\n",
    "        Kiiara +=1\n",
    "    elif entry == 'Kodak Black':\n",
    "        Kodak_Black +=1\n",
    "    elif entry == 'Kygo & Selena Gomez':\n",
    "        Kygo_and_Selena_Gomez +=1\n",
    "    elif entry == 'Lady Gaga':\n",
    "        Lady_Gaga +=1\n",
    "    elif entry == 'Liam Payne':\n",
    "        Liam_Payne +=1\n",
    "    elif entry == 'Lil Dicky':\n",
    "        Lil_Dicky +=1\n",
    "    elif entry == 'Lil Pump':\n",
    "        Lil_Pump +=1\n",
    "    elif entry == 'Lil Uzi Vert':\n",
    "        Lil_Uzi_Vert +=1\n",
    "\n",
    "    elif entry == 'Lil Wayne':\n",
    "        Lil_Wayne +=1\n",
    "    elif entry == 'Lin-Manuel Miranda':\n",
    "        Lin_Manuel_Miranda +=1\n",
    "    elif entry == 'Logic':\n",
    "        Logic +=1\n",
    "    elif entry == 'Lorde':\n",
    "        Lorde +=1\n",
    "    elif entry == 'Luis Fonsi & Daddy Yankee':\n",
    "        Luis_Fonsi_and_Daddy_Yankee +=1\n",
    "    elif entry == 'Lukas Graham':\n",
    "        Lukas_Graham +=1\n",
    "    elif entry == 'MAX':\n",
    "        MAX +=1\n",
    "    elif entry == 'Machine Gun Kelly & Camila Cabello':\n",
    "        Machine_Gun_Kelly_and_Camila_Cabello +=1\n",
    "    elif entry == 'Major Lazer':\n",
    "        Major_Lazer +=1\n",
    "    elif entry == 'Major Lazer & DJ Snake':\n",
    "        Major_Lazer_and_DJ_Snake +=1\n",
    "    elif entry == 'Mariah Carey':\n",
    "        Mariah_Carey +=1\n",
    "    elif entry == 'Marian Hill':\n",
    "        Marian_Hill +=1\n",
    "    elif entry == 'Mark Ronson':\n",
    "        Mark_Ronson +=1\n",
    "    elif entry == 'Maroon 5':\n",
    "        Maroon_5 +=1\n",
    "    elif entry == 'Marshmello & Anne-Marie':\n",
    "        Marshmello_and_Anne_Marie +=1\n",
    "    elif entry == 'Martin Garrix & Bebe Rexha':\n",
    "        Martin_Garrix_and_Bebe_Rexha +=1\n",
    "    elif entry == 'Meghan Trainor':\n",
    "        Meghan_Trainor +=1\n",
    "    elif entry == 'Migos':\n",
    "        Migos +=1\n",
    "    elif entry == 'Mike Posner':\n",
    "        Mike_Posner +=1\n",
    "    elif entry == 'Miley Cyrus':\n",
    "        Miley_Cyrus +=1\n",
    "    elif entry == 'NF':\n",
    "        NF +=1\n",
    "    elif entry == 'Niall Horan':\n",
    "        Niall_Horan +=1\n",
    "    elif entry == 'Nick Jonas':\n",
    "        Nick_Jonas +=1\n",
    "    elif entry == 'Nicki Minaj':\n",
    "        Nicki_Minaj +=1\n",
    "    elif entry == 'Offset & Metro Boomin':\n",
    "        Offset_and_Metro_Boomin +=1\n",
    "    elif entry == 'P!nk':\n",
    "        Pink +=1\n",
    "    elif entry == 'Pentatonix':\n",
    "        Pentatonix +=1\n",
    "    elif entry == 'Portugal. The Man':\n",
    "        Portugal_The_Man +=1\n",
    "    elif entry == 'Post Malone':\n",
    "        Post_Malone +=1\n",
    "    elif entry == 'Prince':\n",
    "        Prince +=1\n",
    "    elif entry == 'Prince And The Revolution':\n",
    "        Prince_And_The_Revolution +=1\n",
    "    elif entry == 'Rae Sremmurd':\n",
    "        Rae_Sremmurd +=1\n",
    "    elif entry == 'Rihanna':\n",
    "        Rihanna +=1\n",
    "    elif entry == 'Ruth B':\n",
    "        Ruth_B +=1\n",
    "    elif entry == 'Sam Hunt':\n",
    "        Sam_Hunt +=1\n",
    "    elif entry == 'Sam Smith':\n",
    "        Sam_Smith +=1\n",
    "    elif entry == 'Selena Gomez':\n",
    "        Selena_Gomez +=1\n",
    "    elif entry == 'Shawn Mendes':\n",
    "        Shawn_Mendes +=1\n",
    "    elif entry == 'Shawn Mendes & Camila Cabello':\n",
    "        Shawn_Mendes_and_Camila_Cabello +=1\n",
    "    elif entry == 'Sia':\n",
    "        Sia +=1\n",
    "    elif entry == 'Taylor Swift':\n",
    "        Taylor_Swift +=1\n",
    "    elif entry == 'The Carters':\n",
    "        The_Carters +=1\n",
    "    elif entry == 'The Chainsmokers':\n",
    "        The_Chainsmokers +=1\n",
    "    elif entry == 'The Chainsmokers & Coldplay':\n",
    "        The_Chainsmokers_and_Coldplay +=1\n",
    "    elif entry == 'The Weeknd':\n",
    "        The_Weeknd +=1\n",
    "    elif entry == 'The Weeknd & Kendrick Lamar':\n",
    "        The_Weeknd_and_Kendrick_Lamar +=1\n",
    "    elif entry == 'Thomas Rhett':\n",
    "        Thomas_Rhett +=1\n",
    "    elif entry == 'Tory Lanez':\n",
    "        Tory_Lanez +=1\n",
    "    elif entry == 'Travis Scott':\n",
    "        Travis_Scott +=1\n",
    "    elif entry == 'Troye Sivan':\n",
    "        Troye_Sivan +=1\n",
    "    elif entry == 'Tyga':\n",
    "        Tyga +=1\n",
    "    elif entry == 'X Ambassadors':\n",
    "        X_Ambassadors +=1\n",
    "    elif entry == 'XXXTENTACION':\n",
    "        XXXTENTACION +=1\n",
    "    elif entry == 'Yo Gotti':\n",
    "        Yo_Gotti +=1\n",
    "    elif entry == 'Young M.A':\n",
    "        Young_MA +=1\n",
    "    elif entry == 'Zara Larsson & MNEK':\n",
    "        Zara_Larsson_and_MNEK +=1\n",
    "    elif entry == 'Zay Hilfigerrr & Zayion McCall':\n",
    "        Zay_Hilfigerrr_and_Zayion_McCall +=1\n",
    "    elif entry == 'Zayn':\n",
    "        Zayn +=1\n",
    "    elif entry == 'Zayn / Taylor Swift':\n",
    "        Zayn_and_Taylor_Swift +=1\n",
    "    elif entry == 'Zedd':\n",
    "        Zedd +=1\n",
    "    elif entry == 'Zedd & Alessia Cara':\n",
    "        Zedd_and_Alessia_Cara +=1\n",
    "    elif entry == 'gnash':\n",
    "        gnash +=1\n",
    "    else:  \n",
    "        twenty_one_pilots +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "\n",
    "        \n",
    "for entry in group25[\"Artist_cleaned\"]:\n",
    "    if entry == '21 Savage': \n",
    "        counts.append(twentyone_Savage)  \n",
    "    elif entry == '5 Seconds Of Summer':\n",
    "        counts.append(five_Seconds_Of_Summer) \n",
    "    elif entry == 'Adele':\n",
    "        counts.append(Adele)   \n",
    "    elif entry == 'Alessia Cara':\n",
    "        counts.append(Alessia_Cara)   \n",
    "    elif entry == 'Amine':\n",
    "        counts.append(Amine)  \n",
    "    elif entry == 'Ariana Grande':\n",
    "        counts.append(Ariana_Grande)   \n",
    "    elif entry == 'Ayo & Teo':\n",
    "        counts.append(Ayo_Teo)    \n",
    "    elif entry == 'Bazzi':\n",
    "        counts.append(Bazzi)       \n",
    "    elif entry == 'Bebe Rexha & Florida Georgia Line':\n",
    "        counts.append(Bebe_Rexha_and_Florida_Georgia_Line)        \n",
    "    elif entry == 'Beyonce':\n",
    "        counts.append(Beyonce)       \n",
    "    elif entry == 'Big Sean':\n",
    "        counts.append(Big_Sean)        \n",
    "    elif entry == 'BlocBoy JB':\n",
    "        counts.append(BlocBoy_JB)  \n",
    "    elif entry == 'Brett Young':\n",
    "        counts.append(Brett_Young) \n",
    "    elif entry == 'Britney Spears':\n",
    "        counts.append(Britney_Spears) \n",
    "    elif entry == 'Bruno Mars':\n",
    "        counts.append(Bruno_Mars)\n",
    "    elif entry == 'Bruno Mars & Cardi B':\n",
    "        counts.append(Bruno_Mars_and_Cardi_B) \n",
    "    elif entry == 'Bryson Tiller':\n",
    "        counts.append(Bryson_Tiller) \n",
    "    elif entry == 'Calvin Harris':\n",
    "        counts.append(Calvin_Harris)\n",
    "    elif entry == 'Camila Cabello':\n",
    "        counts.append(Camila_Cabello)\n",
    "    elif entry == 'Cardi B':\n",
    "        counts.append(Cardi_B)\n",
    "    elif entry == 'Charlie Puth':\n",
    "        counts.append(Charlie_Puth)\n",
    "    elif entry == 'Childish Gambino':\n",
    "        counts.append(Childish_Gambino)\n",
    "    elif entry == 'Chris Brown':\n",
    "        counts.append(Chris_Brown)\n",
    "    elif entry == 'Clean Bandit':\n",
    "        counts.append(Clean_Bandit)\n",
    "    elif entry == 'Coldplay':\n",
    "        counts.append(Coldplay)\n",
    "    elif entry == 'D.R.A.M.':\n",
    "        counts.append(DRAM )\n",
    "    elif entry == 'DJ Khaled':\n",
    "        counts.append(DJ_Khaled)\n",
    "    elif entry == 'DJ Snake':\n",
    "        counts.append(DJ_Snake)\n",
    "    elif entry == 'DNCE':\n",
    "        counts.append(DNCE)\n",
    "    elif entry == 'Dan + Shay':\n",
    "        counts.append(Dan_and_Shay)\n",
    "        \n",
    "    elif entry == 'David Guetta':\n",
    "        counts.append(David_Guetta)\n",
    "    elif entry == 'Daya':\n",
    "        counts.append(Daya)\n",
    "    elif entry == 'Demi Lovato':\n",
    "        counts.append(Demi_Lovato)\n",
    "    elif entry == 'Desiigner':\n",
    "        counts.append(Desiigner)\n",
    "    elif entry == 'Drake':\n",
    "        counts.append(Drake)\n",
    "    elif entry == 'Drake & Future':\n",
    "        counts.append(Drake_and_Future)\n",
    "    elif entry == 'Dua Lipa':\n",
    "        counts.append(Dua_Lipa)\n",
    "    elif entry == 'Ed Sheeran':\n",
    "        counts.append(Ed_Sheeran)\n",
    "    elif entry == 'Ella Mai':\n",
    "        counts.append(Ella_Mai)\n",
    "    elif entry == 'Elle King':\n",
    "        counts.append(Elle_King)\n",
    "    elif entry == 'Ellie Goulding':\n",
    "        counts.append(Ellie_Goulding)\n",
    "    elif entry == 'Eminem':\n",
    "        counts.append(Eminem)\n",
    "    elif entry == 'Fetty Wap':\n",
    "        counts.append(Fetty_Wap)\n",
    "    elif entry == 'Fifth Harmony':\n",
    "        counts.append(Fifth_Harmony)\n",
    "    elif entry == 'Flo Rida':\n",
    "        counts.append(Flo_Rida)\n",
    "    elif entry == 'Florida Georgia Line':\n",
    "        counts.append(Florida_Georgia_Line)\n",
    "    elif entry == 'Flume':\n",
    "        counts.append(Flume)\n",
    "    elif entry == 'French Montana':\n",
    "        counts.append(French_Montana)\n",
    "    elif entry == 'Future':\n",
    "        counts.append(Future)\n",
    "    elif entry == 'G-Eazy':\n",
    "        counts.append(G_Eazy)\n",
    "    elif entry == 'G-Eazy & Halsey':\n",
    "        counts.append(G_Eazy_and_Halsey)\n",
    "    elif entry == 'G-Eazy x Bebe Rexha':\n",
    "        counts.append(G_Eazy_x_Bebe_Rexha)\n",
    "    elif entry == 'Gucci Mane':\n",
    "        counts.append(Gucci_Mane)\n",
    "    elif entry == 'Hailee Steinfeld & Grey':\n",
    "        counts.append(Hailee_Steinfeld_and_Grey)\n",
    "    \n",
    "    elif entry == 'Halsey':\n",
    "        counts.append(Halsey)\n",
    "    elif entry == 'Harry Styles':\n",
    "        counts.append(Harry_Styles)\n",
    "    elif entry == 'Imagine Dragons':\n",
    "        counts.append(Imagine_Dragons)\n",
    "    elif entry == 'J Balvin & Willy William':\n",
    "        counts.append(J_Balvin_and_Willy_William)\n",
    "    elif entry == 'J. Cole':\n",
    "        counts.append(J_Cole)\n",
    "    elif entry == 'James Arthur':\n",
    "        counts.append(James_Arthur)\n",
    "    elif entry == 'James Bay':\n",
    "        counts.append(James_Bay)\n",
    "    \n",
    "    elif entry == 'Jeremih':\n",
    "        counts.append(Jeremih)\n",
    "    elif entry == 'John Legend':\n",
    "        counts.append(John_Legend)\n",
    "    elif entry == 'Jon Bellion':\n",
    "        counts.append(Jon_Bellion)\n",
    "    elif entry == 'Jordan Smith':\n",
    "        counts.append(Jordan_Smith)\n",
    "    elif entry == 'Juice WRLD':\n",
    "        counts.append(Juice_WRLD)\n",
    "    elif entry == 'Julia Michaels':\n",
    "        counts.append(Julia_Michaels)\n",
    "    elif entry == 'Justin Bieber':\n",
    "        counts.append(Justin_Bieber)\n",
    "    elif entry == 'Justin Timberlake':\n",
    "        counts.append(Justin_Timberlake)\n",
    "    elif entry == 'KYLE':\n",
    "        counts.append(KYLE )\n",
    "    elif entry == 'Kane Brown':\n",
    "        counts.append(Kane_Brown)\n",
    "    elif entry == 'Kanye West':\n",
    "        counts.append(Kanye_West)\n",
    "    elif entry == 'Katy Perry':\n",
    "        counts.append(Katy_Perry)\n",
    "    elif entry == 'Keith Urban':\n",
    "        counts.append(Keith_Urban)\n",
    "    elif entry == 'Kelly Clarkson':\n",
    "        counts.append(Kelly_Clarkson)\n",
    "    elif entry == 'Kendrick Lamar':\n",
    "        counts.append(Kendrick_Lamar)\n",
    "    elif entry == 'Kendrick Lamar & SZA':\n",
    "        counts.append(Kendrick_Lamar_and_SZA )\n",
    "    elif entry == 'Kent Jones':\n",
    "        counts.append(Kent_Jones)\n",
    "    elif entry == 'Kesha':\n",
    "        counts.append(Kesha )\n",
    "    elif entry == 'Kevin Gates':\n",
    "        counts.append(Kevin_Gates )\n",
    "    elif entry == 'Khalid':\n",
    "        counts.append(Khalid )\n",
    "    elif entry == 'Khalid & Normani':\n",
    "         counts.append(Khalid_and_Normani )\n",
    "    elif entry == 'Kiiara':\n",
    "        counts.append(Kiiara)\n",
    "    elif entry == 'Kodak Black':\n",
    "        counts.append(Kodak_Black )\n",
    "    elif entry == 'Kygo & Selena Gomez':\n",
    "        counts.append(Kygo_and_Selena_Gomez)\n",
    "    elif entry == 'Lady Gaga':\n",
    "        counts.append(Lady_Gaga)\n",
    "    elif entry == 'Liam Payne':\n",
    "        counts.append(Liam_Payne)\n",
    "    elif entry == 'Lil Dicky':\n",
    "        counts.append(Lil_Dicky)\n",
    "    elif entry == 'Lil Pump':\n",
    "        counts.append(Lil_Pump)\n",
    "    elif entry == 'Lil Uzi Vert':\n",
    "        counts.append(Lil_Uzi_Vert)\n",
    "\n",
    "    elif entry == 'Lil Wayne':\n",
    "        counts.append(Lil_Wayne)\n",
    "    elif entry == 'Lin-Manuel Miranda':\n",
    "        counts.append(Lin_Manuel_Miranda)\n",
    "    elif entry == 'Logic':\n",
    "        counts.append(Logic)\n",
    "    elif entry == 'Lorde':\n",
    "        counts.append(Lorde)\n",
    "    elif entry == 'Luis Fonsi & Daddy Yankee':\n",
    "        counts.append(Luis_Fonsi_and_Daddy_Yankee)\n",
    "    elif entry == 'Lukas Graham':\n",
    "        counts.append(Lukas_Graham )\n",
    "    elif entry == 'MAX':\n",
    "        counts.append(MAX )\n",
    "    elif entry == 'Machine Gun Kelly & Camila Cabello':\n",
    "        counts.append(Machine_Gun_Kelly_and_Camila_Cabello)\n",
    "    elif entry == 'Major Lazer':\n",
    "        counts.append(Major_Lazer)\n",
    "    elif entry == 'Major Lazer & DJ Snake':\n",
    "        counts.append(Major_Lazer_and_DJ_Snake)\n",
    "    elif entry == 'Mariah Carey':\n",
    "        counts.append(Mariah_Carey )\n",
    "    elif entry == 'Marian Hill':\n",
    "        counts.append(Marian_Hill)\n",
    "    elif entry == 'Mark Ronson':\n",
    "        counts.append(Mark_Ronson)\n",
    "    elif entry == 'Maroon 5':\n",
    "        counts.append(Maroon_5 )\n",
    "    elif entry == 'Marshmello & Anne-Marie':\n",
    "        counts.append(Marshmello_and_Anne_Marie )\n",
    "    elif entry == 'Martin Garrix & Bebe Rexha':\n",
    "        counts.append(Martin_Garrix_and_Bebe_Rexha)\n",
    "    elif entry == 'Meghan Trainor':\n",
    "        counts.append(Meghan_Trainor)\n",
    "    elif entry == 'Migos':\n",
    "        counts.append(Migos)\n",
    "    elif entry == 'Mike Posner':\n",
    "        counts.append(Mike_Posner)\n",
    "    elif entry == 'Miley Cyrus':\n",
    "        counts.append(Miley_Cyrus)\n",
    "    elif entry == 'NF':\n",
    "        counts.append(NF)\n",
    "    elif entry == 'Niall Horan':\n",
    "        counts.append(Niall_Horan)\n",
    "    elif entry == 'Nick Jonas':\n",
    "        counts.append(Nick_Jonas)\n",
    "    elif entry == 'Nicki Minaj':\n",
    "        counts.append(Nicki_Minaj)\n",
    "    elif entry == 'Offset & Metro Boomin':\n",
    "        counts.append(Offset_and_Metro_Boomin)\n",
    "    elif entry == 'P!nk':\n",
    "        counts.append(Pink )\n",
    "    elif entry == 'Pentatonix':\n",
    "        counts.append(Pentatonix )\n",
    "    elif entry == 'Portugal. The Man':\n",
    "        counts.append(Portugal_The_Man)\n",
    "    elif entry == 'Post Malone':\n",
    "        counts.append(Post_Malone)\n",
    "    elif entry == 'Prince':\n",
    "        counts.append(Prince)\n",
    "    elif entry == 'Prince And The Revolution':\n",
    "        counts.append(Prince_And_The_Revolution)\n",
    "    elif entry == 'Rae Sremmurd':\n",
    "        counts.append(Rae_Sremmurd)\n",
    "    elif entry == 'Rihanna':\n",
    "        counts.append(Rihanna)\n",
    "    elif entry == 'Ruth B':\n",
    "        counts.append(Ruth_B )\n",
    "    elif entry == 'Sam Hunt':\n",
    "        counts.append(Sam_Hunt)\n",
    "    elif entry == 'Sam Smith':\n",
    "        counts.append(Sam_Smith)\n",
    "    elif entry == 'Selena Gomez':\n",
    "        counts.append(Selena_Gomez)\n",
    "    elif entry == 'Shawn Mendes':\n",
    "        counts.append(Shawn_Mendes)\n",
    "    elif entry == 'Shawn Mendes & Camila Cabello':\n",
    "        counts.append(Shawn_Mendes_and_Camila_Cabello)\n",
    "    elif entry == 'Sia':\n",
    "        counts.append(Sia)\n",
    "    elif entry == 'Taylor Swift':\n",
    "        counts.append(Taylor_Swift)\n",
    "    elif entry == 'The Carters':\n",
    "        counts.append(The_Carters )\n",
    "    elif entry == 'The Chainsmokers':\n",
    "        counts.append(The_Chainsmokers)\n",
    "    elif entry == 'The Chainsmokers & Coldplay':\n",
    "        counts.append(The_Chainsmokers_and_Coldplay)\n",
    "    elif entry == 'The Weeknd':\n",
    "        counts.append(The_Weeknd)\n",
    "    elif entry == 'The Weeknd & Kendrick Lamar':\n",
    "        counts.append(The_Weeknd_and_Kendrick_Lamar)\n",
    "    elif entry == 'Thomas Rhett':\n",
    "        counts.append(Thomas_Rhett)\n",
    "    elif entry == 'Tory Lanez':\n",
    "        counts.append(Tory_Lanez)\n",
    "    elif entry == 'Travis Scott':\n",
    "        counts.append(Travis_Scott)\n",
    "    elif entry == 'Troye Sivan':\n",
    "        counts.append(Troye_Sivan)\n",
    "    elif entry == 'Tyga':\n",
    "        counts.append(Tyga)\n",
    "    elif entry == 'X Ambassadors':\n",
    "        counts.append(X_Ambassadors)\n",
    "    elif entry == 'XXXTENTACION':\n",
    "        counts.append(XXXTENTACION )\n",
    "    elif entry == 'Yo Gotti':\n",
    "        counts.append(Yo_Gotti)\n",
    "    elif entry == 'Young M.A':\n",
    "        counts.append(Young_MA)\n",
    "    elif entry == 'Zara Larsson & MNEK':\n",
    "        counts.append(Zara_Larsson_and_MNEK)\n",
    "    elif entry == 'Zay Hilfigerrr & Zayion McCall':\n",
    "        counts.append(Zay_Hilfigerrr_and_Zayion_McCall)\n",
    "    elif entry == 'Zayn':\n",
    "        counts.append(Zayn)\n",
    "    elif entry == 'Zayn / Taylor Swift':\n",
    "        counts.append(Zayn_and_Taylor_Swift)\n",
    "    elif entry == 'Zedd':\n",
    "        counts.append(Zedd)\n",
    "    elif entry == 'Zedd & Alessia Cara':\n",
    "        counts.append(Zedd_and_Alessia_Cara)\n",
    "    elif entry == 'gnash':\n",
    "        counts.append(gnash)\n",
    "    else:  \n",
    "        counts.append(twenty_one_pilots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CountVectorizer Model\"></a>\n",
    "## CountVectorizer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Can you classify songs into groups based on the vocabulary used within a song? \n",
    "\n",
    "Namely, do certain songs rank in the Top 25 if they contain words about love, versus words about violence and betrayal that might rank lower in the Top 75 - 100 group. \n",
    "\n",
    "In order to eliminate some of the noise associated with words limits on  several hyper parameters were set within the the CountVectorizer. The number of features was limited to 3000, a word needed to appear at least 20 times to be considered as a word, English stop words were ignored and n grams of (1,2) were considered. \n",
    "\n",
    "A Support Vector Classifier was chosen to model the CountVectorizer data with a C of 6. By setting a high C, the model would be harsh on outliers and create  narrow margins in order to be able to differentiate on common words used across all groups. The final model was a SVC with an rbf kernel and a C = 6, an accuracy score of 33% was obtained (26% baseline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33766233766233766\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"181716_edacleaned.csv\",sep=\"\\t\")\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "X = df[\"Lyrics\"]\n",
    "y = df[\"Groups\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y)\n",
    "\n",
    "cvec = CountVectorizer(token_pattern='\\w+',stop_words=\"english\", ngram_range = (1,2),\n",
    "                      decode_error=\"ignore\",max_features=30000, min_df=20) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = cvec.fit_transform(X_train)\n",
    "X_test = cvec.transform(X_test)\n",
    "\n",
    "svc = SVC(6)\n",
    "svc.probability = True #in order to be able to create an ROC curve \n",
    "svc.fit(X_train,y_train)\n",
    "svc_model = svc.score(X_test,y_test)\n",
    "print(svc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more closely at the data I found that 70% of the top 20 words across all the groups were exactly the same. Therefore, the SVC model was unable to differentiate between the words of different groups despite the narrow margins. Due to the underlying similarity in the data across each groups, PCA was not explored. However more features, genres and artist popularity, were added to the model in order to create wider identifying characteristics of each group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CountVectorizer, Artist Popularity & Genre Model\"></a>\n",
    "### CountVectorizer, Artist Popularity & Genre Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"181716_edacleaned_sent_rep_ALLFRI.csv\",sep=\"\\t\")\n",
    "\n",
    "df_count = df[['Lyrics','General_G2','Artist_Top25_Count', 'Artist_Top25_50_Count',\n",
    "       'Artist_Top50_75_Count', 'Artist_Top75_100_Count',\"Groups\"]]\n",
    "\n",
    "#Dummify the genres labels \n",
    "\n",
    "df_count_dummy = pd.get_dummies(df_count,columns=['General_G2'])\n",
    "\n",
    "X = df_count_dummy[[\"Lyrics\",'Artist_Top25_Count', 'Artist_Top25_50_Count',\n",
    "       'Artist_Top50_75_Count', 'Artist_Top75_100_Count','General_G2_ christmas',\n",
    "       'General_G2_ funk', 'General_G2_alternative', 'General_G2_christmas',\n",
    "       'General_G2_country', 'General_G2_electronic', 'General_G2_folk',\n",
    "       'General_G2_hip hop', 'General_G2_indie', 'General_G2_jazz',\n",
    "       'General_G2_latin', 'General_G2_metal', 'General_G2_musicals',\n",
    "       'General_G2_pop', 'General_G2_punk', 'General_G2_r&b', 'General_G2_rap',\n",
    "       'General_G2_reggae', 'General_G2_rock', 'General_G2_soul',\n",
    "       'General_G2_traditional pop', 'General_G2_tropical',\n",
    "       'General_G2_tv music']]\n",
    "\n",
    "columns_ss = X.iloc[:,1:].columns\n",
    "\n",
    "#Standardize the data \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_ss =ss.fit_transform(X.iloc[:,1:])\n",
    "X_ss_df = pd.DataFrame(X_ss,columns=columns_ss)\n",
    "X_ss_df.head(1)\n",
    "\n",
    "X_f = pd.concat([X[\"Lyrics\"], X_ss_df], axis=1)\n",
    "X_f\n",
    "\n",
    "y = df_count_dummy[\"Groups\"]\n",
    "\n",
    "#Train test split \n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X_f,y,test_size=0.2,stratify=y)\n",
    "\n",
    "#Apply CVEC \n",
    "\n",
    "cvec = CountVectorizer(token_pattern='\\w+',stop_words=\"english\", ngram_range = (1,2),\n",
    "                      decode_error=\"ignore\",max_features=50000, min_df=20) \n",
    "\n",
    "Xtrain_l = cvec.fit_transform(Xtrain[\"Lyrics\"]) \n",
    "Xtrain_l = Xtrain_l.toarray()\n",
    "\n",
    "Xtest_l = cvec.transform(Xtest[\"Lyrics\"])\n",
    "Xtest_l = Xtest_l.toarray()\n",
    "\n",
    "Xtrain_l = pd.DataFrame(Xtrain_l)\n",
    "Xtrain_l.head(1)\n",
    "\n",
    "Train_AG = Xtrain.iloc[:,1:]\n",
    "Train_AG.reset_index(inplace=True)\n",
    "Train_AG.head()\n",
    "\n",
    "#Xtrain with CVEC, Artist Popularity, Genre \n",
    "Xtrain_final =pd.concat([Xtrain_l, Train_AG], axis=1)\n",
    "\n",
    "Xtest_l = pd.DataFrame(Xtest_l)\n",
    "Xtest_l.head(1)\n",
    "\n",
    "Test_AG = Xtest.iloc[:,1:]\n",
    "Test_AG.reset_index(inplace=True)\n",
    "Test_AG.head()\n",
    "\n",
    "\n",
    "#Xtest with CVEC, Artist Popularity, Genre \n",
    "Xtest_final =pd.concat([Xtest_l, Test_AG], axis=1)\n",
    "Xtest_final.head(1)\n",
    "\n",
    "#Modeling \n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain_final,ytrain)\n",
    "logreg.score(Xtest_final,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TD-IDF Model\"></a>\n",
    "## TD-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Can you exploit the repetition of particular words and use uncommon words to better classify the songs? \n",
    "\n",
    "In order to eliminate some of the noise associated with words within the TF-IDF the number of features were limited to 3000 and n grams of (1,2) were considered.\n",
    "\n",
    "A Logistic Regression model, proved the best model at classifying songs based on TF-IDF yielding and accuracy score of 29% (3% above baseline). This model is poorer than the CountVectorizer model used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"181716_edacleaned.csv\",sep=\"\\t\")\n",
    "\n",
    "X = df[\"Lyrics\"]\n",
    "y = df[\"Groups\"]\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2,stratify=y)\n",
    "\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english',\n",
    "                        max_features=3000,ngram_range=(1,2))\n",
    "\n",
    "tvec.fit(Xtrain)\n",
    "X_train = tvec.transform(Xtrain)\n",
    "X_test = tvec.transform(Xtest)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train,ytrain)\n",
    "logreg.score(X_test,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closely at the data I found that many of the top 20 words in each group from the TF-IDF were the same words that appeared in the CountVectorizer, due to the number of times they occur in the different songs. In addition, 50% of the top 20 words in each group were exactly the same. This resulted in a poor model only 3% above baseline. \n",
    "\n",
    "In order to expand the features space of the model, genres and artist popularity were added to the TF-IDF features space. This improved the model significantly to an accuracy score of 99.5%. Looking more closely at the highest coefficients within the regression, Artist Popularity seemed to be giving the heaviest weight within the model. In order to see the predictive ability of the actual NLP features of the songs, Artist Popularity was removed from future models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TF-IDF, Artist Popularity & Genre Model\"></a>\n",
    "### TF-IDF, Artist Popularity & Genre Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df[['Lyrics','General_G2','Artist_Top25_Count', 'Artist_Top25_50_Count',\n",
    "       'Artist_Top50_75_Count', 'Artist_Top75_100_Count',\"Groups\"]]\n",
    "\n",
    "df_count_dummy = pd.get_dummies(df_count,columns=['General_G2'])\n",
    "\n",
    "X = df_count_dummy[[\"Lyrics\",'Artist_Top25_Count', 'Artist_Top25_50_Count',\n",
    "       'Artist_Top50_75_Count', 'Artist_Top75_100_Count','General_G2_ christmas',\n",
    "       'General_G2_ funk', 'General_G2_alternative', 'General_G2_christmas',\n",
    "       'General_G2_country', 'General_G2_electronic', 'General_G2_folk',\n",
    "       'General_G2_hip hop', 'General_G2_indie', 'General_G2_jazz',\n",
    "       'General_G2_latin', 'General_G2_metal', 'General_G2_musicals',\n",
    "       'General_G2_pop', 'General_G2_punk', 'General_G2_r&b', 'General_G2_rap',\n",
    "       'General_G2_reggae', 'General_G2_rock', 'General_G2_soul',\n",
    "       'General_G2_traditional pop', 'General_G2_tropical',\n",
    "       'General_G2_tv music']]\n",
    "\n",
    "columns_ss = X.iloc[:,1:].columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_ss =ss.fit_transform(X.iloc[:,1:])\n",
    "X_ss_df = pd.DataFrame(X_ss,columns=columns_ss)\n",
    "X_ss_df.head(1)\n",
    "\n",
    "X_f = pd.concat([X[\"Lyrics\"], X_ss_df], axis=1)\n",
    "X_f\n",
    "\n",
    "y = df_count_dummy[\"Groups\"]\n",
    "\n",
    "#Train-Test Split \n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X_f,y,test_size=0.2,stratify=y)\n",
    "\n",
    "\n",
    "#Apply TF-IDF\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english',\n",
    "                       #sublinear_tf=True,\n",
    "                       #max_df=0.5)\n",
    "                       max_features=3000,ngram_range=(1,2))\n",
    "\n",
    "\n",
    "\n",
    "Xtrain_l = tvec.fit_transform(Xtrain[\"Lyrics\"]) \n",
    "Xtrain_l = Xtrain_l.toarray()\n",
    "\n",
    "Xtest_l = tvec.transform(Xtest[\"Lyrics\"])\n",
    "Xtest_l = Xtest_l.toarray()\n",
    "\n",
    "Xtrain_l = pd.DataFrame(Xtrain_l,columns=[tvec.get_feature_names()])\n",
    "\n",
    "\n",
    "Train_AG = Xtrain.iloc[:,1:]\n",
    "Train_AG.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "#This is the training set to fit with model \n",
    "Xtrain_final =pd.concat([Xtrain_l, Train_AG], axis=1)\n",
    "\n",
    "Xtest_l = pd.DataFrame(Xtest_l,columns=[tvec.get_feature_names()])\n",
    "\n",
    "\n",
    "Test_AG = Xtest.iloc[:,1:]\n",
    "Test_AG.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "#This is the testing set to fit with model \n",
    "Xtest_final =pd.concat([Xtest_l, Test_AG], axis=1)\n",
    "\n",
    "\n",
    "#GRIDSEARCHCV\n",
    "\n",
    "params = {\"C\":np.logspace(-7,2,100)}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(logreg,params,verbose=1,cv=10)\n",
    "grid.fit(Xtrain_final,ytrain)\n",
    "\n",
    "#Getting the best model from the GridSearchCV\n",
    "optimal_model = grid.best_estimator_\n",
    "\n",
    "optimal_model.score(Xtest_final,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the largest coefficients in the model \n",
    "\n",
    "coefs = pd.DataFrame(dict(coef=optimal_model.coef_[0],\n",
    "                                     abscoef=np.abs(optimal_model.coef_[0]),\n",
    "                                     feature=Xtest_final.columns))\n",
    "coefs.sort_values('abscoef', ascending=False, inplace=True)\n",
    "coefs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Sentiment Analysis\"></a>\n",
    "## Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Create a improved classifier model using the sentiment of songs. \n",
    "\n",
    "With commutes, radio's and commuters, are more likely to listen to happier songs than sad ones. Therefore, happier songs should appear within the Top 25, Top 25-50 groups more than later groups.\n",
    "\n",
    "Vader sentiment analysis toolkit was used in order to create the sentiment for each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_neg'] = 0\n",
    "df['vader_pos'] = 0\n",
    "df['vader_neu'] = 0\n",
    "df['vader_compound'] = 0   \n",
    "\n",
    "for i, q in enumerate(df[\"Lyrics\"]):\n",
    "    vs = analyzer.polarity_scores(q)\n",
    "    df.iloc[i, -4] = vs['neg']\n",
    "    df.iloc[i, -3] = vs['pos']\n",
    "    df.iloc[i, -2] = vs['neu']\n",
    "    df.iloc[i, -1] = vs['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Sentiment KNN Model\"></a>\n",
    "### Sentiment KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an initial review of the sentiment scores across the different groups, no clear difference in the positive or negative sentiment could be seen across the groups. A key reason for this was that the rhythm plays an important role in giving tone. Many songs had very happy lyrics, but in actual fact have a sad rhythm. This juxtaposition was not picked up in the sentiment analysis and caused no significant difference to be seen across the groups. Nevertheless, a KNN model looking at the 5 nearest neighbours was used in order to classify songs based on sentiment into groups.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"181716_edacleaned_sentiment.csv\",sep=\"\\t\")\n",
    "\n",
    "X = df[['vader_neg', 'vader_pos', 'vader_neu', 'vader_compound']]\n",
    "y = df[\"Groups\"]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_ss = ss.fit_transform(X)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_ss,y,test_size=0.2,stratify=y)\n",
    "\n",
    "\n",
    "#GRIDSEARCH to find optimal number of neighbours\n",
    "\n",
    "params = {\"n_neighbors\":[5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,90,100]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "grid = GridSearchCV(knn,params,verbose=1,cv=10)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal model found in the Gridsearch\n",
    "optimal_model = grid.best_estimator_\n",
    "optimal_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Compound-Sentiment-&-Genres-Model\"></a>\n",
    "### Compound Sentiment & Genres Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closely at the data, compound sentiment proved to be the most diversifying feature across the groups. Genres, was added into the model to weight the composition of the different groups in order to create a better classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"181716_edacleaned_sent_rep_ALLFRI.csv\",sep=\"\\t\")\n",
    "df_dum = pd.get_dummies(df,columns=[\"General_G2\"])\n",
    "\n",
    "y = df[\"Groups\"]\n",
    "X_data  = df_dum[['vader_compound',\n",
    "       'General_G2_ christmas', 'General_G2_ funk',\n",
    "       'General_G2_alternative', 'General_G2_christmas', 'General_G2_country',\n",
    "       'General_G2_electronic', 'General_G2_folk', 'General_G2_hip hop',\n",
    "       'General_G2_indie', 'General_G2_jazz', 'General_G2_latin',\n",
    "       'General_G2_metal', 'General_G2_musicals', 'General_G2_pop',\n",
    "       'General_G2_punk', 'General_G2_r&b', 'General_G2_rap',\n",
    "       'General_G2_reggae', 'General_G2_rock', 'General_G2_soul',\n",
    "       'General_G2_traditional pop', 'General_G2_tropical',\n",
    "       'General_G2_tv music']]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X_data)\n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2,stratify=y)\n",
    "\n",
    "\n",
    "#GRIDSEARCH to find optimal hyperparameters \n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "params = {\"solver\":[\"newton-cg\",\"sag\",\"saga\",\"lbfgs\"],\n",
    "          \"C\":[1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "grid = GridSearchCV(logreg,params,cv=5,verbose=1)\n",
    "grid.fit(Xtrain,ytrain)\n",
    "\n",
    "optimal_grid = grid.best_estimator_\n",
    "optimal_grid.score(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Repetitiveness\"></a>\n",
    "## Repetitiveness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Exploit the genre-breakdown within each group in order better classify songs? \n",
    "    \n",
    "Pop songs are known to be more repetitive than rap songs. Using the genre-composition of each of the group, can you use a percentage of repetition feature that will be able to better classify songs.\n",
    "\n",
    "To calculate the percentage of repetition the formula used was: (length of the song - length of the compression)/ length of the song. \n",
    "\n",
    "In order to asses the amount of repetition there was in a song the Zopfli compression alogorithim was used. The Zopfli compression algorithim uses Huffman coding and Lempel Ziv under the hood. Lempel Ziv, seeks to match words in a song to previous words in a song and will replace it with a \"marker\", resulting in only non-repetitive words being left. Huffman coding counts the frequency of each letter in a body of text and creates shorter bit encoding for letters that appear more often, resulting in a more compressed document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_compression = []\n",
    "percent_compress = []\n",
    "\n",
    "for song in df[\"Lyrics\"]:\n",
    "    song_byt = str.encode(song)\n",
    "    \n",
    "    c = zopfli.ZopfliDeflater()\n",
    "    z = c.compress(song_byt) + c.flush()\n",
    "    \n",
    "    length_compression.append(len(z))\n",
    "    \n",
    "    len_song = len(song)\n",
    "    len_compress = len(z)\n",
    "    calc = (len_song - len_compress)/len_song\n",
    "    \n",
    "    percent_compress.append(calc)\n",
    "    \n",
    "df[\"percent_compress\"] = percent_compress\n",
    "df [\"length_compression\"] = length_compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Part-of-Speech-Tagging\"></a>\n",
    "## Part of Speech Tagging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: Look at the construct of the lyrics to see if there is a difference in the number of nouns, adjectives and other part of speech tags used across the different groups.  \n",
    "\n",
    "NLTK's toolkit was used to get the part of speech tags for every songs. For each part of speech tag a new column was created for that part of speech tag, with a count of the tag for every song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating count of different Part of Speech Tags in every song\n",
    "\n",
    "quote_final_values = [] \n",
    "openbracket_final_values = [] \n",
    "closebracket_final_values = []\n",
    "comma_final_values = []\n",
    "tire_final_values = []\n",
    "doublepoints_final_values = []\n",
    "openquotes_final_values = []\n",
    "dot_final_values = []\n",
    "cc_final_values = [] \n",
    "cd_final_values = []\n",
    "dt_final_values = []\n",
    "ex_final_values = []\n",
    "fw_final_values = []\n",
    "in_final_values = []\n",
    "jj_final_values = []\n",
    "jjr_final_values = []\n",
    "jjs_final_values = []\n",
    "ls_final_values = []\n",
    "md_final_values = []\n",
    "nn_final_values = []\n",
    "nnp_final_values = []\n",
    "nnps_final_values = []\n",
    "nns_final_values = []\n",
    "pdt_final_values = []\n",
    "pos_final_values = []\n",
    "prp_final_values = []\n",
    "prpdollar_final_values = []\n",
    "rb_final_values = []\n",
    "rbr_final_values = []\n",
    "rp_final_values = []\n",
    "sym_final_values = []\n",
    "to_final_values = []\n",
    "uh_final_values = []\n",
    "vb_final_values = []\n",
    "vbd_final_values = []\n",
    "vbg_final_values = []\n",
    "vbn_final_values = []\n",
    "vbp_final_values = []\n",
    "vbz_final_values = []\n",
    "wdt_final_values = []\n",
    "wp_final_values = []\n",
    "wpdollar_final_values = []\n",
    "wrb_final_values = []\n",
    "none_final_valuess = []\n",
    "\n",
    "\n",
    "\n",
    "for song in df[\"Lyrics\"]:\n",
    "    t_sentence = []\n",
    "    for lyrics in song.split(\" \"):\n",
    "        tags = pos_tag(tok.tokenize(lyrics)) #here we get all the tags \n",
    "        \n",
    "        for t in tags: #loop through the tags, to only get the part of speech \n",
    "            t1 = t[1]\n",
    "            \n",
    "            t_sentence.append(t1) #append it to our list\n",
    "    quote_count = 0 \n",
    "    openbracket_count = 0 \n",
    "    closebracket_count = 0\n",
    "    comma_count = 0\n",
    "    tire_count = 0\n",
    "    doublepoints_count = 0\n",
    "    openquotes_count = 0\n",
    "    dot_count = 0\n",
    "    cc_count = 0 \n",
    "    cd_count = 0\n",
    "    dt_count = 0\n",
    "    ex_count = 0\n",
    "    fw_count = 0\n",
    "    in_count = 0\n",
    "    jj_count = 0\n",
    "    jjr_count = 0\n",
    "    jjs_count = 0\n",
    "    ls_count = 0\n",
    "    md_count = 0\n",
    "    nn_count = 0\n",
    "    nnp_count = 0\n",
    "    nnps_count = 0\n",
    "    nns_count = 0\n",
    "    pdt_count = 0\n",
    "    pos_count = 0\n",
    "    prp_count = 0\n",
    "    prpdollar_count = 0\n",
    "    rb_count = 0\n",
    "    rbr_count = 0\n",
    "    rp_count = 0\n",
    "    sym_count = 0\n",
    "    to_count = 0\n",
    "    uh_count = 0\n",
    "    vb_count = 0\n",
    "    vbd_count = 0\n",
    "    vbg_count = 0\n",
    "    vbn_count = 0\n",
    "    vbp_count = 0\n",
    "    vbz_count = 0\n",
    "    wdt_count = 0\n",
    "    wp_count = 0\n",
    "    wpdollar_count = 0\n",
    "    wrb_count = 0\n",
    "    none_count = 0\n",
    "\n",
    "    for entry in t_sentence:\n",
    "        #print(entry)\n",
    "        if entry == \"''\":\n",
    "            quote_count +=1 \n",
    "        elif entry =='(':\n",
    "            openbracket_count +=1 \n",
    "        elif entry ==')':\n",
    "            closebracket_count+=1  \n",
    "        elif entry ==',':\n",
    "            comma_count+=1 \n",
    "        elif entry =='--':\n",
    "            tire_count+=1 \n",
    "        elif entry ==':':\n",
    "            doublepoints_count+=1\n",
    "        elif entry =='``$':\n",
    "            openquotes_count+=1 \n",
    "        elif entry =='.':\n",
    "            dot_count+=1 \n",
    "        elif entry =='CC':\n",
    "            cc_count+=1\n",
    "        elif entry =='CD':\n",
    "            cd_count+=1\n",
    "        elif entry =='DT':\n",
    "            dt_count+=1\n",
    "        elif entry =='EX':\n",
    "            ex_count+=1\n",
    "        elif entry =='FW':\n",
    "            fw_count+=1\n",
    "        elif entry =='IN':\n",
    "            in_count+=1\n",
    "        elif entry =='JJ':\n",
    "            jj_count+=1 \n",
    "        elif entry =='JJR':\n",
    "            jjr_count+=1    \n",
    "        elif entry =='JJS':\n",
    "             jjs_count+=1\n",
    "        elif entry =='LS':\n",
    "             ls_count+=1      \n",
    "        elif entry =='MD':\n",
    "            md_count+=1  \n",
    "        elif entry =='NN':\n",
    "            nn_count+=1 \n",
    "        elif entry =='NNP':\n",
    "            nnp_count+=1    \n",
    "        elif entry =='NNPS':\n",
    "            nnps_count+=1   \n",
    "        elif entry =='NNS':\n",
    "             nns_count+=1 \n",
    "        elif entry =='PDT':\n",
    "            pdt_count+=1 \n",
    "        elif entry =='POS':\n",
    "            pos_count+=1 \n",
    "        elif entry =='PRP':\n",
    "            prp_count+=1 \n",
    "        elif entry =='PRP$':\n",
    "            prpdollar_count+=1 \n",
    "        elif entry =='RB':\n",
    "            rb_count+=1 \n",
    "        elif entry =='RBR':\n",
    "            rbr_count+=1 \n",
    "        elif entry =='RP':\n",
    "            rp_count+=1 \n",
    "        elif entry =='SYM':\n",
    "            sym_count+=1 \n",
    "        elif entry =='TO':\n",
    "             to_count+=1\n",
    "        elif entry =='UH':\n",
    "            uh_count+=1 \n",
    "        elif entry =='VB':\n",
    "            vb_count+=1 \n",
    "        elif entry =='VBD':\n",
    "            vbd_count+=1     \n",
    "        elif entry =='VBG':\n",
    "            vbg_count+=1 \n",
    "        elif entry =='VBN':\n",
    "            vbn_count+=1\n",
    "        elif entry =='VBP':\n",
    "            vbp_count+=1\n",
    "        elif entry =='VBZ':\n",
    "            vbz_count+=1\n",
    "        elif entry =='WDT':\n",
    "            wdt_count+=1\n",
    "        elif entry =='WP':\n",
    "            wp_count+=1\n",
    "        elif entry =='WP$':\n",
    "            wpdollar_count+=1\n",
    "        elif entry =='WRB':\n",
    "            wrb_count+=1\n",
    "        else:\n",
    "            none_count+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    quote_final_values.append(quote_count) \n",
    "    openbracket_final_values.append(openbracket_count) \n",
    "    closebracket_final_values.append(closebracket_count)\n",
    "    comma_final_values.append(comma_count)\n",
    "    tire_final_values.append(tire_count)\n",
    "    doublepoints_final_values.append(doublepoints_count)\n",
    "    openquotes_final_values.append(openquotes_count)\n",
    "    dot_final_values.append(dot_count)\n",
    "    cc_final_values.append(cc_count) \n",
    "    cd_final_values.append(cd_count)\n",
    "    dt_final_values.append(dt_count)\n",
    "    ex_final_values.append(ex_count)\n",
    "    fw_final_values.append(fw_count)\n",
    "    in_final_values.append(in_count)\n",
    "    jj_final_values.append(jj_count)\n",
    "    jjr_final_values.append(jjr_count)\n",
    "    jjs_final_values.append(jjs_count)\n",
    "    ls_final_values.append(ls_count)\n",
    "    md_final_values.append(md_count)\n",
    "    nn_final_values.append(nn_count)\n",
    "    nnp_final_values.append(nnp_count)\n",
    "    nnps_final_values.append(nnps_count)\n",
    "    nns_final_values.append(nns_count)\n",
    "    pdt_final_values.append(pdt_count)\n",
    "    pos_final_values.append(pos_count)\n",
    "    prp_final_values.append(prp_count)\n",
    "    prpdollar_final_values.append(prpdollar_count)\n",
    "    rb_final_values.append(rb_count)\n",
    "    rbr_final_values.append(rbr_count)\n",
    "    rp_final_values.append(rp_count)\n",
    "    sym_final_values.append(sym_count)\n",
    "    to_final_values.append(to_count)\n",
    "    uh_final_values.append(uh_count)\n",
    "    vb_final_values.append(vb_count)\n",
    "    vbd_final_values.append(vbd_count)\n",
    "    vbg_final_values.append(vbg_count)\n",
    "    vbn_final_values.append(vbn_count)\n",
    "    vbp_final_values.append(vbp_count)\n",
    "    vbz_final_values.append(vbz_count)\n",
    "    wdt_final_values.append(wdt_count)\n",
    "    wp_final_values.append(wp_count)\n",
    "    wpdollar_final_values.append(wpdollar_count)\n",
    "    wrb_final_values.append(wrb_count)\n",
    "    none_final_valuess.append(none_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Part of Speech Tags to the DataFrame (new columns)\n",
    "\n",
    "df[\"quote\"] = quote_final_values\n",
    "df[\"openbracker\"] = openbracket_final_values  \n",
    "df[\"closebracket\"] = closebracket_final_values \n",
    "df[\"comma\"] = comma_final_values  \n",
    "df[\"tire\"] = tire_final_values \n",
    "df[\"doublepoints\"] = doublepoints_final_values  \n",
    "df[\"openquotes\"] = openquotes_final_values  \n",
    "df[\"dot\"] = dot_final_values \n",
    "df[\"cc\"] = cc_final_values   \n",
    "df[\"cd\"] = cd_final_values  \n",
    "df[\"dt\"] = dt_final_values  \n",
    "df[\"ex\"] = ex_final_values  \n",
    "df[\"fw\"] = fw_final_values  \n",
    "df[\"in\"] = in_final_values  \n",
    "df[\"jj\"] = jj_final_values  \n",
    "df[\"jjr\"] = jjr_final_values  \n",
    "df[\"jjs\"] = jjs_final_values  \n",
    "df[\"ls\"] = ls_final_values  \n",
    "df[\"md\"] = md_final_values  \n",
    "df[\"nn\"] = nn_final_values  \n",
    "df[\"nnp\"] = nnp_final_values  \n",
    "df[\"nnps\"] = nnps_final_values  \n",
    "df[\"nns\"] = nns_final_values  \n",
    "df[\"pdt\"] = pdt_final_values  \n",
    "df[\"pos\"] = pos_final_values  \n",
    "df[\"prp\"] = prp_final_values  \n",
    "df[\"prpdollar\"] = prpdollar_final_values  \n",
    "df[\"rb\"] = rb_final_values  \n",
    "df[\"rbr\"] = rbr_final_values  \n",
    "df[\"rp\"] = rp_final_values  \n",
    "df[\"sym\"] = sym_final_values  \n",
    "df[\"to\"] = to_final_values  \n",
    "df[\"uh\"] = uh_final_values  \n",
    "df[\"vb\"] = vb_final_values  \n",
    "df[\"vbd\"] = vbd_final_values  \n",
    "df[\"vbg\"] = vbg_final_values  \n",
    "df[\"vbn\"] = vbn_final_values  \n",
    "df[\"vbp\"] = vbp_final_values  \n",
    "df[\"vbz\"] = vbz_final_values  \n",
    "df[\"wdt\"] = wdt_final_values  \n",
    "df[\"wp\"] = wp_final_values  \n",
    "df[\"wpdollar\"] = wpdollar_final_values  \n",
    "df[\"wrb\"] = wrb_final_values  \n",
    "df[\"none\"] = none_final_valuess  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Percentage-Repetition,-Part-of-Speech-Tags-&-Genre-Model\"></a>\n",
    "### Percentage Repetition, Part of Speech Tags & Genre Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting different models I noticed that some features simply added noise to the model resulting in a poorer classifier whilst others created a stronger classifier. Part of speech tags, Percentage of Repetition and Genres  were significantly different across each group and therefore combined in a model. Using a Logistic Regression and accuracy score of 40% was achieved, the best model to classify songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating various dataframes \n",
    "df = pd.read_csv(\"181716_edacleaned_sent_rep_ALLFRI.csv\",sep=\"\\t\")\n",
    "df2 = pd.read_csv(\"181716_edacleaned_ALL.csv\",sep=\"\\t\")\n",
    "sub_df2 = df2[['quote', 'openbracker', 'closebracket',\n",
    "       'comma', 'tire', 'doublepoints', 'openquotes', 'dot', 'cc', 'cd', 'dt',\n",
    "       'ex', 'fw', 'in', 'jj', 'jjr', 'jjs', 'ls', 'md', 'nn', 'nnp', 'nnps',\n",
    "       'nns', 'pdt', 'pos', 'prp', 'prpdollar', 'rb', 'rbr', 'rp', 'sym', 'to',\n",
    "       'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wpdollar',\n",
    "       'wrb', 'none']]\n",
    "\n",
    "DF = pd.concat([df, sub_df2], axis=1)\n",
    "\n",
    "#Getting the Dummy variables \n",
    "DF_dum = pd.get_dummies(DF,columns=[\"General_G2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = DF_dum[\"Groups\"]\n",
    "X = DF_dum[['vader_compound','percent_compress', 'quote', 'openbracker', 'closebracket',\n",
    "       'comma', 'tire', 'doublepoints', 'openquotes', 'dot', 'cc', 'cd', 'dt',\n",
    "       'ex', 'fw', 'in', 'jj', 'jjr', 'jjs', 'ls', 'md', 'nn', 'nnp', 'nnps',\n",
    "       'nns', 'pdt', 'pos', 'prp', 'prpdollar', 'rb', 'rbr', 'rp', 'sym', 'to',\n",
    "       'uh', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wpdollar',\n",
    "       'wrb', 'none', 'General_G2_ christmas', 'General_G2_ funk',\n",
    "       'General_G2_alternative', 'General_G2_christmas', 'General_G2_country',\n",
    "       'General_G2_electronic', 'General_G2_folk', 'General_G2_hip hop',\n",
    "       'General_G2_indie', 'General_G2_jazz', 'General_G2_latin',\n",
    "       'General_G2_metal', 'General_G2_musicals', 'General_G2_pop',\n",
    "       'General_G2_punk', 'General_G2_r&b', 'General_G2_rap',\n",
    "       'General_G2_reggae', 'General_G2_rock', 'General_G2_soul',\n",
    "       'General_G2_traditional pop', 'General_G2_tropical',\n",
    "       'General_G2_tv music']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_ss = ss.fit_transform(X)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_ss,y,test_size=0.2,stratify=y)\n",
    "\n",
    "#Apply modeling \n",
    "\n",
    "svc = SVC(C=1.06)\n",
    "svc.probability = True\n",
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"All-Features-Model\"></a>\n",
    "### All Features Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final model, I wanted to combine all of the NLP features created from the Lyrics into a model. With a Random Forest Decision Tree Classifier, the highest accuracy score of 99% was achieved. Nevertheless, as previously seen, the weight of the model stems from the Artist Popularity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"181716_edacleaned_ALL.csv\",sep=\"\\t\")\n",
    "\n",
    "df2 = pd.read_csv(\"181716_edacleaned_sent_rep_ALLFRI.csv\",sep=\"\\t\")\n",
    "artist_df = df2[['Artist_Top25_Count', 'Artist_Top25_50_Count','Artist_Top50_75_Count', 'Artist_Top75_100_Count']]\n",
    "\n",
    "DF = pd.concat([df, artist_df ], axis=1)\n",
    "df_dum = pd.get_dummies(DF,columns=[\"General_G2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_dum[\"Groups\"]\n",
    "\n",
    "X_art = df_dum[['percent_compress', \n",
    "       'vader_neg', 'vader_pos', 'vader_neu', 'vader_compound', 'quote',\n",
    "       'openbracker', 'closebracket', 'comma', 'tire', 'doublepoints',\n",
    "       'openquotes', 'dot', 'cc', 'cd', 'dt', 'ex', 'fw', 'in', 'jj', 'jjr',\n",
    "       'jjs', 'ls', 'md', 'nn', 'nnp', 'nnps', 'nns', 'pdt', 'pos', 'prp',\n",
    "       'prpdollar', 'rb', 'rbr', 'rp', 'sym', 'to', 'uh', 'vb', 'vbd', 'vbg',\n",
    "       'vbn', 'vbp', 'vbz', 'wdt', 'wp', 'wpdollar', 'wrb', 'none',\n",
    "       'Artist_Top25_Count', 'Artist_Top25_50_Count', 'Artist_Top50_75_Count',\n",
    "       'Artist_Top75_100_Count', 'General_G2_ christmas', 'General_G2_ funk',\n",
    "       'General_G2_alternative', 'General_G2_christmas', 'General_G2_country',\n",
    "       'General_G2_electronic', 'General_G2_folk', 'General_G2_hip hop',\n",
    "       'General_G2_indie', 'General_G2_jazz', 'General_G2_latin',\n",
    "       'General_G2_metal', 'General_G2_musicals', 'General_G2_pop',\n",
    "       'General_G2_punk', 'General_G2_r&b', 'General_G2_rap',\n",
    "       'General_G2_reggae', 'General_G2_rock', 'General_G2_soul',\n",
    "       'General_G2_traditional pop', 'General_G2_tropical',\n",
    "       'General_G2_tv music']]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_ss = ss.fit_transform(X_art)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_ss,y,test_size=0.2,stratify=y)\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Conclusion\"></a>\n",
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the conclusion of the project and more details on the accuracy scores obtained with various models please see the \"NLP Billboard100 Presentation\" PDF in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
